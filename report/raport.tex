\documentclass[12pt,a4paper]{article}

% Polskie znaki i formatowanie
\usepackage[utf8]{inputenc}
\usepackage[polish]{babel}
\usepackage{lmodern}
\usepackage[T1]{fontenc}

% Pakiety graficzne i tabele
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{array}

% Kolory i linki
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
}

% Matematyka i kod
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{textcomp}

% Ustawienia listingów (kod) - POPRAWIONE Z KOLORAMI
\lstset{
    language=Python,
    basicstyle=\small\ttfamily,
    breaklines=true,
    frame=single,
    numbers=left,
    numberstyle=\tiny\color{gray},
    numbersep=8pt,
    tabsize=4,
    showstringspaces=false,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{green!50!black}\itshape,
    stringstyle=\color{red!70!black},
    emphstyle=\color{purple},
    backgroundcolor=\color{gray!10},
    rulecolor=\color{black!30},
    xleftmargin=15pt,
    framexleftmargin=15pt
}

% Marginesy
\usepackage[left=2.5cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}

% Nagłówki i stopki
\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\rhead{\thepage}
\lhead{Projekt: Implementacja i analiza sieci neuronowych MLP}

% Metadane dokumentu
\title{
    \Huge{\textbf{Projekt: Implementacja i analiza\\sieci neuronowych MLP}} \\
    \vspace{0.5cm}
    \Large{Porównanie implementacji ręcznej i frameworkowej}
}

\author{
    Jakub Sornat \\
    Maciej Tajs \\
    Bartłomiej Sadza
}

\date{\today}

\begin{document}

\maketitle
\thispagestyle{empty}

\newpage
\tableofcontents
\newpage

% ============================================================================
\section{Wstęp}
% ============================================================================

Wielowarstwowe sieci perceptronów (MLP) są fundamentalnym narzędziem uczenia maszynowego, łączącym biologiczne inspiracje z solidnym aparatem matematycznym. Ich skuteczność w zadaniach klasyfikacji i regresji czyni je nieodzownym elementem praktycznych zastosowań: od analizy finansowej przez rozpoznawanie obrazów po systemy rekomendacji.

Niniejszy raport porównuje dwa podejścia implementacyjne MLP: ręczne budowanie algorytmów od podstaw oraz wykorzystanie frameworków. Takie zestawienie umożliwia dogłębne zrozumienie zarówno teoretycznych podstaw sieci, jak i praktycznych aspektów ich zastosowania.

\subsection{Cel projektu}

Celem projektu jest implementacja wielowarstwowego perceptronu (MLP) w dwóch wariantach:
\begin{itemize}
    \item \textbf{Implementacja ręczna} -- napisana od podstaw w NumPy, z pełną kontrolą nad forward propagation, backward propagation i optymalizacją
    \item \textbf{Implementacja frameworkowa} -- wykorzystująca bibliotekę TensorFlow/Keras
\end{itemize}

Projekt ma na celu:
\begin{enumerate}
    \item Zrozumienie wewnętrznego działania sieci neuronowych poprzez implementację od podstaw
    \item Porównanie wydajności i dokładności obu podejść
    \item Praktyczne zastosowanie MLP do problemów klasyfikacji i regresji
    \item Przeprowadzenie systematycznych eksperymentów z różnymi hiperparametrami
\end{enumerate}

\subsection{Zakres prac}

W ramach projektu zrealizowano:
\begin{itemize}
    \item Implementację ręczną sieci MLP z warstwami Dense, aktywacjami ReLU i Softmax
    \item Implementację modelu MLP w Keras/TensorFlow
    \item Preprocessing i przygotowanie 4 zbiorów danych (2 klasyfikacyjne, 2 regresyjne)
    \item Przeprowadzenie eksperymentów z grid search po hiperparametrach
    \item Analizę wyników i wizualizację (learning curves, confusion matrices, wykresy porównawcze)
    \item Dokumentację techniczną projektu
\end{itemize}

% ============================================================================
\section{Opis wykorzystanych narzędzi i metod}
% ============================================================================

\subsection{Środowisko programistyczne}

Projekt realizowano w środowisku Python (wersja 3.12.7), wykorzystując kluczowe biblioteki:
\begin{itemize}
    \item \textbf{NumPy} -- wydajna implementacja operacji algebraicznych (wersja 2.1.1)
    \item \textbf{TensorFlow/Keras} -- framework deep learning (wersja 2.18.0)
    \item \textbf{Matplotlib} -- profesjonalna wizualizacja danych
    \item \textbf{Pandas} -- przetwarzanie struktur danych tabelarycznych (wersja 2.2.3)
    \item \textbf{scikit-learn} -- preprocessing i metryki ewaluacji (wersja 1.5.2)
    \item \textbf{OpenPyXL} -- eksport wyników do Excel
\end{itemize}

\subsection{Kluczowe metody}

\subsubsection{Grid Search}

Systematyczne przeszukiwanie przestrzeni hiperparametrów obejmowało:
\begin{itemize}
    \item Liczba warstw ukrytych: \{1, 2, 3, 4\}
    \item Liczba neuronów: \{8, 16, 32, 64\}
    \item Learning rate: \{0.001, 0.005, 0.01, 0.02\}
    \item Batch size: 32 (stały)
    \item Liczba epok: 50 (stała)
\end{itemize}

Łącznie przetestowano 64 kombinacje hiperparametrów dla każdej konfiguracji.

\subsubsection{Metryki ewaluacji}

\paragraph{Klasyfikacja:}
\begin{itemize}
    \item \textbf{Accuracy} -- odsetek poprawnych klasyfikacji
    \item \textbf{Precision} -- dokładność predykcji pozytywnej
    \item \textbf{Recall} -- czułość na klasę pozytywną
    \item \textbf{F1-score} -- harmoniczna średnia precision i recall
\end{itemize}

\paragraph{Regresja:}
\begin{itemize}
    \item \textbf{MSE} (Mean Squared Error) -- średni błąd kwadratowy
    \item \textbf{MAE} (Mean Absolute Error) -- średni błąd bezwzględny
    \item \textbf{R² score} -- współczynnik determinacji
\end{itemize}

% ============================================================================
\section{Problemy badawcze}
% ============================================================================

\subsection{Pytania badawcze}

W projekcie postawiono następujące pytania badawcze:

\begin{enumerate}
    \item \textbf{Porównanie implementacji:} Czy ręczna implementacja MLP może osiągnąć porównywalne wyniki do profesjonalnego frameworka (Keras)?

    \item \textbf{Wpływ architektury:} Jak liczba warstw ukrytych i neuronów wpływa na jakość predykcji?

    \item \textbf{Learning rate:} Jaki jest optymalny learning rate dla różnych typów problemów?

    \item \textbf{Generalizacja:} Czy model dobrze generalizuje na danych testowych (problem overfittingu)?

    \item \textbf{Typ problemu:} Czy wyniki różnią się między klasyfikacją a regresją?
\end{enumerate}

\subsection{Hipotezy}

\begin{enumerate}
    \item Keras osiągnie lepsze wyniki ze względu na optymalizacje niskopoziomowe i zaawansowane optymalizatory
    \item Głębsze sieci (więcej warstw) będą lepsze dla złożonych wzorców
    \item Mniejszy learning rate da stabilniejszy, ale wolniejszy trening
    \item Implementacja ręczna będzie miała problemy z konwergencją dla skomplikowanych danych
\end{enumerate}

% ============================================================================
\section{Wykorzystane dane}
% ============================================================================

W projekcie wykorzystano pięć zbiorów danych: dwa do klasyfikacji binarnej, dwa do regresji, oraz jeden do klasyfikacji obrazów (wieloklasowej).

\subsection{Adult Income Dataset (Klasyfikacja)}

\textbf{Źródło:} UCI Machine Learning Repository

\textbf{Opis:} Przewidywanie, czy osoba zarabia powyżej 50K\$ rocznie.

\textbf{Charakterystyka:}
\begin{itemize}
    \item Liczba rekordów: 32,561
    \item Liczba cech (po preprocessingu): 21
    \item Klasy: 2 (>50K, $\leq$50K)
    \item Typ: Klasyfikacja binarna
\end{itemize}

\textbf{Preprocessing:}
\begin{itemize}
    \item Usunięcie brakujących wartości
    \item Binary encoding: sex, native\_country
    \item Target encoding: occupation
    \item One-hot encoding: marital\_status, relationship, workclass
    \item Feature engineering: capital\_net\_log
    \item Undersampling klas do proporcji 1:1
\end{itemize}

\subsection{Loan Approval Dataset (Klasyfikacja)}

\textbf{Opis:} Zatwierdzanie kredytów bankowych.

\textbf{Charakterystyka:}
\begin{itemize}
    \item Liczba rekordów: 50,000
    \item Liczba cech (po preprocessingu): 9
    \item Klasy: 2 (Approved, Rejected)
    \item Typ: Klasyfikacja binarna
\end{itemize}

\textbf{Preprocessing:}
\begin{itemize}
    \item Selekcja cech o korelacji > 0.15 z targetem
    \item Transformacja logarytmiczna zmiennych skośnych
    \item Undersampling i standaryzacja
\end{itemize}

\subsection{Stock Market Dataset (Regresja)}

\textbf{Opis:} Predykcja cen akcji na podstawie wskaźników technicznych.

\textbf{Charakterystyka:}
\begin{itemize}
    \item Liczba rekordów: 29,900
    \item Liczba cech: 5
    \item Zmienna docelowa: Cena zamknięcia
    \item Typ: Regresja (szereg czasowy)
\end{itemize}

\textbf{Preprocessing:}
\begin{itemize}
    \item Zachowanie uporządkowania czasowego
    \item Standaryzacja wszystkich wskaźników
    \item Time-based split: 70/15/15
\end{itemize}

\subsection{Student Performance Dataset (Regresja)}

\textbf{Opis:} Przewidywanie wyników egzaminów na podstawie czynników socjoekonomicznych.

\textbf{Charakterystyka:}
\begin{itemize}
    \item Liczba rekordów: 6,607
    \item Liczba cech (po preprocessingu): 5
    \item Zmienna docelowa: Wynik egzaminu
    \item Typ: Regresja
\end{itemize}

\textbf{Preprocessing:}
\begin{itemize}
    \item Ordinal encoding zmiennych porządkowych
    \item Binary encoding zmiennych kategorycznych
    \item Selekcja cech: $|r| > 0.15$
\end{itemize}

\subsection{Fashion MNIST Dataset (Klasyfikacja obrazów)}

\textbf{Źródło:} Zalando Research

\textbf{Opis:} Klasyfikacja 10 kategorii odzieży na podstawie obrazów.

\textbf{Charakterystyka:}
\begin{itemize}
    \item Liczba obrazów: 70,000 (60k train, 10k test)
    \item Rozmiar obrazu: 28×28 pikseli (784 cechy)
    \item Klasy: 10 (T-shirt, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot)
    \item Typ: Klasyfikacja wieloklasowa
\end{itemize}

\textbf{Preprocessing:}
\begin{itemize}
    \item Normalizacja pikseli do zakresu [0, 1]
    \item Flattening dla MLP (784 cech)
    \item Reshape do 28×28×1 dla CNN
\end{itemize}

% ============================================================================
\section{Architektura sieci neuronowych}
% ============================================================================

\subsection{Architektura ogólna MLP}

Obie implementacje (ręczna i Keras) wykorzystują architekturę MLP:

\begin{itemize}
    \item \textbf{Warstwa wejściowa:} liczba neuronów = liczba cech
    \item \textbf{Warstwy ukryte:} N warstw Dense z aktywacją ReLU
    \item \textbf{Warstwa wyjściowa:}
    \begin{itemize}
        \item Klasyfikacja: Dense(n\_classes) + Softmax
        \item Regresja: Dense(1) + aktywacja liniowa
    \end{itemize}
\end{itemize}

\subsection{Implementacja ręczna}

\subsubsection{Warstwa Dense}

Transformacja liniowa:
\begin{equation}
    \mathbf{y} = \mathbf{W}^T \mathbf{x} + \mathbf{b}
\end{equation}

\textbf{Inicjalizacja wag:} Xavier/Glorot uniform
\begin{equation}
    W \sim \mathcal{U}\left(-\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}}\right)
\end{equation}

\textbf{Backward propagation:}
\begin{align}
    \frac{\partial L}{\partial \mathbf{W}} &= \mathbf{x}^T \frac{\partial L}{\partial \mathbf{y}} \\
    \frac{\partial L}{\partial \mathbf{b}} &= \sum \frac{\partial L}{\partial \mathbf{y}} \\
    \frac{\partial L}{\partial \mathbf{x}} &= \frac{\partial L}{\partial \mathbf{y}} \mathbf{W}^T
\end{align}

\subsubsection{Aktywacja ReLU}

\begin{equation}
    \text{ReLU}(x) = \max(0, x)
\end{equation}

Gradient:
\begin{equation}
    \frac{\partial \text{ReLU}(x)}{\partial x} = \begin{cases}
        1 & \text{jeśli } x > 0 \\
        0 & \text{w przeciwnym razie}
    \end{cases}
\end{equation}

\subsubsection{Aktywacja Softmax}

\begin{equation}
    \text{Softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
\end{equation}

\subsubsection{Funkcje straty}

\textbf{Klasyfikacja (Cross-entropy):}
\begin{equation}
    L = -\frac{1}{N} \sum_{i=1}^{N} \log(\hat{y}_{i,c_i})
\end{equation}

\textbf{Regresja (MSE):}
\begin{equation}
    L = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2
\end{equation}

\subsection{Implementacja Keras}

Implementacja Keras wykorzystuje wysokopoziomowe API:

\begin{lstlisting}[language=Python, caption=Przykładowa architektura MLP w Keras]
model = keras.Sequential()
model.add(layers.Input(shape=(n_inputs,)))

# Warstwy ukryte
for i in range(n_hidden_layers):
    model.add(layers.Dense(
        n_neurons,
        activation='relu',
        kernel_initializer='glorot_uniform'
    ))

# Warstwa wyjsciowa
if task_type == "classification":
    model.add(layers.Dense(n_outputs, activation='softmax'))
    model.compile(
        optimizer=optimizers.SGD(learning_rate=lr),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
else:
    model.add(layers.Dense(1, activation='linear'))
    model.compile(
        optimizer=optimizers.SGD(learning_rate=lr),
        loss='mse',
        metrics=['mae']
    )
\end{lstlisting}

\subsection{Architektury zaawansowane}

\subsubsection{CNN dla Fashion MNIST}

\begin{lstlisting}[language=Python, caption=Architektura CNN dla obrazów]
model = keras.Sequential([
    layers.Input(shape=(28, 28, 1)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])
\end{lstlisting}

\subsubsection{CNN 1D dla szeregów czasowych}

\begin{lstlisting}[language=Python, caption=CNN 1D dla Stock Market]
model = keras.Sequential([
    layers.Input(shape=(n_features, 1)),
    layers.Conv1D(64, 3, activation='relu'),
    layers.MaxPooling1D(2),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(1)
])
\end{lstlisting}

\subsubsection{LSTM dla danych sekwencyjnych}

\begin{lstlisting}[language=Python, caption=LSTM dla Stock Market]
model = keras.Sequential([
    layers.Input(shape=(n_features, 1)),
    layers.LSTM(32, return_sequences=False),
    layers.Dense(32, activation='relu'),
    layers.Dense(1)
])
\end{lstlisting}

% ============================================================================
% WYNIKI EKSPERYMENTÓW - AUTOMATYCZNIE WYGENEROWANE
% ============================================================================

\input{wyniki_generated.tex}

% ============================================================================
\section{Wizualizacje}
% ============================================================================

\subsection{Learning Curves}

Learning curves pokazują postęp treningu modelu (loss i metryka główna przez epoki). Pozwalają zidentyfikować overfitting (gdy validation loss rośnie, a training loss maleje) oraz niedouczenie.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{visualizations/Manual_MLP_-_Classification_(train_val_test)_learning_curves.png}
\caption{Learning curves dla Manual MLP - Adult Income (train/val/test split)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{visualizations/Keras_MLP_-_Classification_(train_val_test)_learning_curves.png}
\caption{Learning curves dla Keras MLP - Adult Income (train/val/test split)}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{visualizations/Manual_MLP_-_Classification_Our_(train_val_test)_learning_curves.png}
\caption{Learning curves dla Manual MLP - Loan Approval}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{visualizations/Keras_MLP_-_Classification_Our_(train_val_test)_learning_curves.png}
\caption{Learning curves dla Keras MLP - Loan Approval}
\end{figure}

\subsection{Confusion Matrices}

Macierze pomyłek pokazują szczegółowy rozkład predykcji dla każdej klasy.

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Manual_MLP_-_Classification_(Test_Set)_confusion_matrix.png}
    \textit{Manual MLP - Adult Income}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Keras_MLP_-_Classification_(Test_Set)_confusion_matrix.png}
    \textit{Keras MLP - Adult Income}
\end{minipage}
\caption{Porównanie confusion matrices dla Adult Income Dataset}
\end{figure}

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Manual_MLP_-_Classification_Our_(Test_Set)_confusion_matrix.png}
    \textit{Manual MLP - Loan Approval}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Keras_MLP_-_Classification_Our_(Test_Set)_confusion_matrix.png}
    \textit{Keras MLP - Loan Approval}
\end{minipage}
\caption{Porównanie confusion matrices dla Loan Approval Dataset}
\end{figure}

\subsection{Wykresy regresji (Predicted vs Actual)}

Scatter plots pokazują korelację między predykcjami a rzeczywistymi wartościami. Idealna predykcja to linia $y = x$.

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Manual_MLP_-_Regression_(Test_Set)_scatter.png}
    \textit{Manual MLP - Stock Market}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Keras_MLP_-_Regression_(Test_Set)_scatter.png}
    \textit{Keras MLP - Stock Market}
\end{minipage}
\caption{Porównanie predykcji dla Stock Market Dataset. Manual MLP wykazuje znaczne odchylenia od idealnej linii (ujemny R²), podczas gdy Keras MLP osiąga niemal idealną korelację (R² = 0.999).}
\end{figure}

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Manual_MLP_-_Regression_Our_(Test_Set)_scatter.png}
    \textit{Manual MLP - Student Performance}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Keras_MLP_-_Regression_Our_(Test_Set)_scatter.png}
    \textit{Keras MLP - Student Performance}
\end{minipage}
\caption{Porównanie predykcji dla Student Performance Dataset}
\end{figure}

\subsection{Fashion MNIST - Klasyfikacja obrazów}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{visualizations/Fashion_MNIST_Keras_CNN_learning_curves.png}
\caption{Learning curves dla CNN na Fashion MNIST - widoczna stabilna konwergencja}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{visualizations/Fashion_MNIST_Keras_CNN_confusion_matrix.png}
\caption{Confusion matrix dla CNN na Fashion MNIST. Model osiąga 92.88\% accuracy, z najlepszą dokładnością dla klas ''Trouser'' i ''Bag'', a najgorszą dla ''Shirt'' (często mylony z ''T-shirt'' i ''Coat'').}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{visualizations/Fashion_MNIST_Keras_MLP_confusion_matrix.png}
\caption{Confusion matrix dla MLP na Fashion MNIST (73.22\% accuracy). Widoczne znacznie więcej błędów klasyfikacji w porównaniu do CNN.}
\end{figure}

\subsection{Zaawansowane modele regresji}

\begin{figure}[H]
\centering
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Stock_Market_CNN1D_scatter.png}
    \textit{CNN 1D}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
    \centering
    \includegraphics[width=\textwidth]{visualizations/Stock_Market_LSTM_scatter.png}
    \textit{LSTM}
\end{minipage}
\caption{Zaawansowane architektury dla Stock Market. Obie osiągają bardzo wysokie R² ($>$0.997), ale Keras MLP wciąż najlepszy.}
\end{figure}

% ============================================================================
\section{Wnioski}
% ============================================================================

\subsection{Odpowiedzi na pytania badawcze}

\textbf{1. Porównanie implementacji:}
\begin{itemize}
    \item Dla \textbf{klasyfikacji tabelarycznej}: wyniki porównywalne (różnica <0.5\%)
    \item Adult Income: Manual 82.97\% vs Keras 82.71\%
    \item Loan Approval: Manual 86.31\% vs Keras 86.30\%
    \item Dla \textbf{regresji}: Keras znacząco lepszy
    \item Stock Market: Keras R²=0.999 vs Manual R²=-1.25
\end{itemize}

\textbf{2. Wpływ architektury:}
\begin{itemize}
    \item Optymalna liczba warstw: 2-4 dla klasyfikacji, 1-2 dla regresji
    \item Optymalna liczba neuronów: 16-32 dla danych tabelarycznych, 64-128 dla obrazów
    \item Zbyt głębokie sieci (>4 warstwy) nie poprawiają wyników dla małych zbiorów
\end{itemize}

\textbf{3. Learning rate:}
\begin{itemize}
    \item Optymalny zakres: 0.001-0.02
    \item Mniejszy LR (0.001) lepszy dla regresji
    \item Większy LR (0.01-0.02) dopuszczalny dla klasyfikacji
\end{itemize}

\textbf{4. Generalizacja:}
\begin{itemize}
    \item Modele nie wykazują znacznego overfittingu
    \item Validation loss stabilizuje się po 20-30 epokach
    \item Undersampling skutecznie balansuje klasy
\end{itemize}

\textbf{5. Typ problemu:}
\begin{itemize}
    \item Klasyfikacja: obie implementacje podobne
    \item Regresja: Keras zdecydowanie lepszy (optymalizatory)
    \item Obrazy: CNN przewyższa MLP o +18.85\%
\end{itemize}

\subsection{Weryfikacja hipotez}

\begin{enumerate}
    \item \textbf{Hipoteza 1 (Keras lepszy):} \textcolor{green!60!black}{CZĘŚCIOWO POTWIERDZONA}
    \begin{itemize}
        \item Prawda dla regresji i obrazów
        \item Fałsz dla prostej klasyfikacji tabelarycznej
    \end{itemize}

    \item \textbf{Hipoteza 2 (głębsze sieci lepsze):} \textcolor{red}{ODRZUCONA}
    \begin{itemize}
        \item 2-3 warstwy wystarczające dla danych tabelarycznych
        \item Więcej warstw nie poprawia wyników
    \end{itemize}

    \item \textbf{Hipoteza 3 (mniejszy LR stabilniejszy):} \textcolor{green!60!black}{POTWIERDZONA}
    \begin{itemize}
        \item LR=0.001 najstabilniejszy
        \item Większy LR może prowadzić do oscylacji
    \end{itemize}

    \item \textbf{Hipoteza 4 (Manual problemy z konwergencją):} \textcolor{green!60!black}{POTWIERDZONA}
    \begin{itemize}
        \item Ujemny R² dla Stock Market
        \item SGD bez momentum niewystarczający
    \end{itemize}
\end{enumerate}

\subsection{Problemy napotkane i rozwiązania}

\begin{enumerate}
    \item \textbf{Konwergencja Manual MLP dla regresji:}
    \begin{itemize}
        \item Problem: Ujemny R² dla Stock Market
        \item Przyczyna: SGD bez momentum/Adam
        \item Rozwiązanie: Użycie Keras z zaawansowanymi optymalizatorami
    \end{itemize}

    \item \textbf{CNN 1D wymiary:}
    \begin{itemize}
        \item Problem: 5 cech to za mało dla wielu warstw Conv
        \item Rozwiązanie: Max 2 warstwy, warunkowy pooling
    \end{itemize}

    \item \textbf{Fashion MNIST czas treningu:}
    \begin{itemize}
        \item Problem: Kilka godzin na pełny grid search
        \item Rozwiązanie: Zwiększenie batch size, zmniejszenie grid
    \end{itemize}
\end{enumerate}

\subsection{Główne wnioski}

\begin{enumerate}
    \item \textbf{Edukacyjna wartość ręcznej implementacji:} Głębokie zrozumienie backpropagation i optymalizacji

    \item \textbf{Praktyczna przewaga frameworków:} Keras oferuje stabilność, szybkość i zaawansowane optymalizatory

    \item \textbf{Dobór architektury:} CNN zdecydowanie lepszy dla obrazów (92.88\% vs 74.03\%)

    \item \textbf{Znaczenie optymalizatora:} Adam/RMSprop przewyższają SGD dla złożonych problemów

    \item \textbf{Preprocessing kluczowy:} Standaryzacja, balansowanie klas i selekcja cech mają duży wpływ
\end{enumerate}

% ============================================================================
\section{Bibliografia}
% ============================================================================

\begin{enumerate}
    \item Goodfellow, I., Bengio, Y., Courville, A. (2016). \textit{Deep Learning}. MIT Press.
    \item Nielsen, M. A. (2015). \textit{Neural Networks and Deep Learning}. Determination Press.
    \item Géron, A. (2019). \textit{Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow}. O'Reilly Media.
    \item TensorFlow Documentation: \url{https://www.tensorflow.org/}
    \item Keras Documentation: \url{https://keras.io/}
    \item UCI Machine Learning Repository: \url{https://archive.ics.uci.edu/ml/}
    \item Xiao, H., Rasul, K., Vollgraf, R. (2017). Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms. arXiv:1708.07747
\end{enumerate}

% ============================================================================
\appendix
\section{Kod źródłowy}
% ============================================================================

Pełny kod źródłowy projektu dostępny jest w repozytorium GitHub:

\url{https://github.com/Sornat11/NeuralNetwork}

\textbf{Struktura projektu:}
\begin{itemize}
    \item \texttt{src/manual\_mlp/} -- Implementacja ręczna (layers.py, model.py, metrics.py)
    \item \texttt{src/models/} -- Implementacja Keras (keras\_mlp.py, keras\_cnn.py, keras\_lstm\_regression.py)
    \item \texttt{data/} -- Zbiory danych i skrypty preprocessingu
    \item \texttt{utils/} -- Narzędzia pomocnicze (experiment\_runner, visualization, results\_exporter)
    \item \texttt{results/} -- Wyniki eksperymentów (.xlsx) i wizualizacje (.png)
    \item \texttt{main.py} -- Główny skrypt eksperymentów MLP
    \item \texttt{main\_keras.py} -- Eksperymenty Keras MLP
    \item \texttt{main\_fashion\_mnist.py} -- Eksperymenty Fashion MNIST
    \item \texttt{main\_regression\_advanced.py} -- CNN 1D i LSTM
\end{itemize}

\end{document}
