# Analiza i porÃ³wnanie wybranych architektur sieci neuronowych

**Analysis and Comparison of Selected Neural Network Architectures for Regression, Classification, and Image Recognition Tasks**

Projekt akademicki porÃ³wnujÄ…cy rÃ³Å¼ne architektury sieci neuronowych (MLP, CNN, RNN/LSTM/GRU) w trzech typach zadaÅ„.

## Autorzy

- **Jakub Sornat**
- **Maciej Tajs**
- **BartÅ‚omiej Sadza**

**Kierunek:** Informatyka i Ekonometria
**ProwadzÄ…cy:** dr inÅ¼. RadosÅ‚aw Puka
**Rok:** 2025

---

## ðŸ“‹ Spis treÅ›ci

- [O projekcie](#o-projekcie)
- [FunkcjonalnoÅ›ci](#funkcjonalnoÅ›ci)
- [Struktura projektu](#struktura-projektu)
- [Instalacja](#instalacja)
- [UÅ¼ycie](#uÅ¼ycie)
- [Eksperymenty](#eksperymenty)
- [Wyniki](#wyniki)
- [Technologie](#technologie)

---

## O projekcie

Projekt realizuje kompleksowÄ… analizÄ™ i porÃ³wnanie wybranych architektur sieci neuronowych dla trzech rodzajÃ³w problemÃ³w:

1. **Problem klasyfikacyjny** - Iris, Wine (wÅ‚asny MLP + Keras MLP)
2. **Problem regresyjny** - Airline Passengers szereg czasowy (MLP + CNN + LSTM/GRU)
3. **Problem analizy obrazÃ³w** - Fashion MNIST (wÅ‚asny MLP + Keras MLP + CNN)

### Kluczowe cechy:

- âœ… **WÅ‚asna implementacja MLP** od zera (NumPy) z backpropagation
- âœ… **Optimizery**: SGD, SGD+Momentum, Adam, RMSprop
- âœ… **Gotowe modele**: CNN, RNN/LSTM/GRU w Keras/TensorFlow
- âœ… **Framework eksperymentalny** z automatycznym grid search
- âœ… **Wielokrotne powtÃ³rzenia** (min. 5x) dla kaÅ¼dego zestawu parametrÃ³w
- âœ… **Metryki**: accuracy, precision, recall, F1, MSE, MAE, RÂ²
- âœ… **Wizualizacje**: learning curves, confusion matrix, porÃ³wnania
- âœ… **PodziaÅ‚ danych**: 80/20 i 70/15/15

---

## FunkcjonalnoÅ›ci

### 1. WÅ‚asna implementacja MLP

```python
from src.manual_mlp.model import Model
from src.manual_mlp.layers import LayerDense
from src.manual_mlp.activations import ActivationReLU
from src.manual_mlp.losses import SoftmaxCategoricalCrossentropy
from src.manual_mlp.optimizers import OptimizerAdam

# Zbuduj model
model = Model()
model.add(LayerDense(4, 64))
model.add(ActivationReLU())
model.add(LayerDense(64, 3))

# Skonfiguruj
model.set(
    loss=SoftmaxCategoricalCrossentropy(),
    optimizer=OptimizerAdam(learning_rate=0.001)
)

# Trenuj
history = model.fit(X_train, y_train, epochs=100, batch_size=32)
```

### 2. Framework eksperymentalny

```python
from experiments.experiment_runner import ExperimentRunner, create_param_grid

runner = ExperimentRunner(results_dir="results")

param_grid = create_param_grid(
    base_params={"epochs": 100, "batch_size": 32},
    variations={
        "n_layers": [1, 2, 3],
        "neurons": [32, 64, 128],
        "learning_rate": [0.001, 0.01, 0.1],
        "optimizer": ["sgd", "adam", "rmsprop"],
    }
)

results = runner.run_experiment(
    experiment_name="iris_classification",
    model_fn=create_model,
    train_fn=train_model,
    eval_fn=evaluate_model,
    data=dataset,
    param_grid=param_grid,
    n_repeats=5,  # Wielokrotne powtÃ³rzenia
)
```

### 3. Wizualizacje

```python
from experiments.visualizations import (
    plot_learning_curves,
    plot_confusion_matrix,
    plot_parameter_comparison,
    plot_model_comparison
)

# Learning curves
plot_learning_curves(history, save_path="results/learning_curves.png")

# PorÃ³wnanie parametrÃ³w
plot_parameter_comparison(
    results_df,
    param_name="learning_rate",
    metrics=["test_accuracy_mean", "test_f1_score_mean"]
)

# PorÃ³wnanie modeli
plot_model_comparison(
    {"Custom MLP": results1, "Keras MLP": results2, "CNN": results3},
    metric="test_accuracy_mean"
)
```

---

## Struktura projektu

```
NeuralNetwork/
â”‚
â”œâ”€â”€ data/                           # ModuÅ‚y do Å‚adowania danych
â”‚   â”œâ”€â”€ datasets.py                 # Åadowanie Iris, Wine, Fashion MNIST, Airline
â”‚   â”œâ”€â”€ preprocessing.py            # Preprocessing
â”‚   â””â”€â”€ sample_data_generator.py    # Generator danych syntetycznych
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ manual_mlp/                 # WÅASNA IMPLEMENTACJA MLP (NumPy)
â”‚   â”‚   â”œâ”€â”€ model.py                # Model z forward/backward pass
â”‚   â”‚   â”œâ”€â”€ layers.py               # LayerDense z backpropagation
â”‚   â”‚   â”œâ”€â”€ activations.py          # ReLU, Softmax, Sigmoid, Linear
â”‚   â”‚   â”œâ”€â”€ losses.py               # Categorical CE, MSE, MAE
â”‚   â”‚   â”œâ”€â”€ optimizers.py           # SGD, Adam, RMSprop
â”‚   â”‚   â””â”€â”€ metrics.py              # Accuracy, Precision, Recall, F1, RÂ²
â”‚   â”‚
â”‚   â””â”€â”€ models/                     # MODELE W KERAS/TENSORFLOW
â”‚       â”œâ”€â”€ multilayer_perceptron.py  # MLP w Keras
â”‚       â”œâ”€â”€ convolutional_nn.py      # CNN w Keras
â”‚       â””â”€â”€ recurrent_nn.py          # RNN/LSTM/GRU w Keras
â”‚
â”œâ”€â”€ experiments/                    # Framework eksperymentalny
â”‚   â”œâ”€â”€ experiment_runner.py        # Runner z grid search i powtÃ³rzeniami
â”‚   â”œâ”€â”€ visualizations.py           # Wizualizacje wynikÃ³w
â”‚   â”œâ”€â”€ run_classification_experiments.py    # Eksperymenty klasyfikacyjne
â”‚   â”œâ”€â”€ run_regression_experiments.py        # Eksperymenty regresyjne
â”‚   â””â”€â”€ run_image_experiments.py             # Eksperymenty na obrazach
â”‚
â”œâ”€â”€ results/                        # Wyniki eksperymentÃ³w (CSV, JSON, PNG)
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ seed.py                     # Ustawianie seed dla reproducibility
â”‚
â”œâ”€â”€ requirements.txt                # ZaleÅ¼noÅ›ci
â”œâ”€â”€ main.py                         # GÅ‚Ã³wny punkt wejÅ›cia
â””â”€â”€ README.md                       # Ten plik
```

---

## Instalacja

### 1. Sklonuj repozytorium

```bash
git clone https://github.com/your-username/NeuralNetwork.git
cd NeuralNetwork
```

### 2. UtwÃ³rz Å›rodowisko wirtualne (zalecane)

**Windows:**
```bash
python -m venv venv
.\venv\Scripts\activate
```

**Linux/Mac:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Zainstaluj zaleÅ¼noÅ›ci

```bash
pip install -r requirements.txt
```

**Wymagane biblioteki:**
- numpy
- pandas
- tensorflow (>=2.10)
- scikit-learn
- matplotlib
- seaborn
- optuna (opcjonalnie)

---

## UÅ¼ycie

### Szybki start

```bash
# Uruchom interaktywne menu
python main.py
```

### Uruchamianie eksperymentÃ³w

#### 1. Eksperymenty klasyfikacyjne (Iris + Wine)

```bash
cd experiments
python run_classification_experiments.py
```

Testuje:
- WÅ‚asny MLP vs Keras MLP
- Parametry: liczba warstw, neurony, learning rate, optimizers, momentum
- 5 powtÃ³rzeÅ„ kaÅ¼dego zestawu parametrÃ³w
- Wyniki zapisywane do `results/`

#### 2. Eksperymenty regresyjne (Airline Passengers)

```bash
cd experiments
python run_regression_experiments.py
```

Testuje:
- MLP vs LSTM vs GRU
- Szeregi czasowe (lookback=12)
- Parametry: warstwy, jednostki, learning rate
- Metryki: MSE, MAE, RÂ²

#### 3. Eksperymenty na obrazach (Fashion MNIST)

```bash
cd experiments
python run_image_experiments.py
```

Testuje:
- WÅ‚asny MLP vs Keras MLP vs CNN
- 10 klas ubraÅ„ (28x28 pikseli)
- Confusion matrix dla najlepszego modelu
- PorÃ³wnanie accuracy

---

## Eksperymenty

### Parametry testowane w projekcie

Zgodnie z wymaganiami projektu, testujemy **minimum 4 wartoÅ›ci kaÅ¼dego parametru**:

| Parametr | WartoÅ›ci testowane |
|----------|-------------------|
| **Liczba warstw** | 1, 2, 3, 4 |
| **Liczba neuronÃ³w** | 32, 64, 128, 256 |
| **Learning rate** | 0.0001, 0.001, 0.01, 0.1 |
| **Optimizer** | SGD, SGD+Momentum, Adam, RMSprop |
| **Momentum** | 0.0, 0.5, 0.9, 0.99 |

### Wielokrotne powtÃ³rzenia

KaÅ¼dy zestaw parametrÃ³w jest trenowany **minimum 5 razy** (zgodnie z wymaganiami), poniewaÅ¼ uczenie sieci nie jest deterministyczne.

### Metryki

**Klasyfikacja:**
- Accuracy
- Precision (macro)
- Recall (macro)
- F1-score (macro)
- Confusion matrix

**Regresja:**
- MSE (Mean Squared Error)
- MAE (Mean Absolute Error)
- RMSE (Root Mean Squared Error)
- RÂ² (Coefficient of Determination)

### Zbiory danych

KaÅ¼dy eksperyment ewaluuje na **trzech zbiorach** (zgodnie z wymaganiami):
- **Train set** - dane treningowe
- **Validation set** - dane walidacyjne (do strojenia)
- **Test set** - dane testowe (do koÅ„cowej ewaluacji)

---

## Wyniki

Wyniki sÄ… automatycznie zapisywane w katalogu `results/`:

```
results/
â”œâ”€â”€ custom_mlp_iris_20250106_143022.csv      # Wyniki w CSV
â”œâ”€â”€ custom_mlp_iris_20250106_143022.json     # Wyniki w JSON
â”œâ”€â”€ keras_mlp_iris_20250106_143530.csv
â”œâ”€â”€ iris_model_comparison.png                # Wykresy porÃ³wnawcze
â”œâ”€â”€ iris_custom_learning_rate.png
â”œâ”€â”€ fashion_mnist_confusion_matrix.png
â””â”€â”€ ...
```

### Format wynikÃ³w CSV

Kolumny zawierajÄ…:
- Parametry modelu (n_layers, neurons, learning_rate, optimizer, momentum)
- Metryki dla **train/val/test** z:
  - `_mean` - Å›rednia z 5+ powtÃ³rzeÅ„
  - `_std` - odchylenie standardowe
  - `_min` - minimalna wartoÅ›Ä‡
  - `_max` - maksymalna wartoÅ›Ä‡
  - `_best` - najlepsza wartoÅ›Ä‡

PrzykÅ‚ad:
```csv
n_layers,neurons,learning_rate,optimizer,test_accuracy_mean,test_accuracy_std,test_accuracy_best
2,64,0.001,adam,0.9533,0.0123,0.9667
2,128,0.001,adam,0.9600,0.0089,0.9733
...
```

---

## Technologie

### WÅ‚asna implementacja (NumPy)

- **Forward propagation** - przejÅ›cie sygnaÅ‚u przez sieÄ‡
- **Backpropagation** - obliczanie gradientÃ³w
- **Optimizery**:
  - SGD (Stochastic Gradient Descent)
  - SGD + Momentum
  - Adam (Adaptive Moment Estimation)
  - RMSprop (Root Mean Square Propagation)
- **Funkcje aktywacji**: ReLU, Softmax, Sigmoid, Linear
- **Funkcje straty**: Categorical Crossentropy, MSE, MAE

### Gotowe modele (Keras/TensorFlow)

- **MLP** - Multilayer Perceptron
- **CNN** - Convolutional Neural Network (Conv2D + Pooling)
- **RNN** - Recurrent Neural Network (LSTM, GRU)
- **Conv1D-LSTM Hybrid** - dla szeregÃ³w czasowych

### NarzÄ™dzia

- **scikit-learn** - podziaÅ‚ danych, metryki, datasety (Iris, Wine)
- **matplotlib + seaborn** - wizualizacje
- **pandas** - zarzÄ…dzanie wynikami
- **optuna** (opcjonalnie) - automatyczna optymalizacja hiperparametrÃ³w

---

## ReprodukowalnoÅ›Ä‡

Wszystkie eksperymenty uÅ¼ywajÄ… `set_seed()` do zapewnienia reprodukowalnoÅ›ci:

```python
from utils.seed import set_seed
set_seed(42)  # Ten sam seed = te same wyniki
```

---

## Sprawozdanie

PeÅ‚ne sprawozdanie projektu dostÄ™pne jest tutaj:

ðŸ“„ [Project Report (DOCX)](https://aghedupl-my.sharepoint.com/:w:/r/personal/jakubsornat_student_agh_edu_pl/Documents/report.docx?d=w719a3c159b694350a6cdfea27e91fec0&csf=1&web=1&e=UyxR3n)

---

## Licencja

Projekt edukacyjny / Educational purposes only

---

## Kontakt

W razie pytaÅ„ skontaktuj siÄ™ z autorami:
- Jakub Sornat
- Maciej Tajs
- BartÅ‚omiej Sadza

---

**Projekt wykonany w ramach kursu "Sieci Neuronowe i Uczenie GÅ‚Ä™bokie"**
**AGH KrakÃ³w, 2025**
